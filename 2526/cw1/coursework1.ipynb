{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Olubusolami-R/gaussian-processes-cw/blob/main/2526/cw1/coursework1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "035f35cb",
      "metadata": {
        "id": "035f35cb"
      },
      "outputs": [],
      "source": [
        "# Ensure correct environment for GPy\n",
        "import numpy, os, signal\n",
        "if numpy.__version__ != \"1.26.4\":\n",
        "    # 1. Uninstall conflicting packages\n",
        "    !pip uninstall -y jax scipy jaxlib paramz pytensor > /dev/null 2>&1\n",
        "    # 2. Install the versions required for GPy\n",
        "    !pip install --force-reinstall numpy==1.26.4 scipy GPy paramz > /dev/null 2>&1\n",
        "    # 3. Restart runtime to load correct NumPy ABI\n",
        "    os.kill(os.getpid(), signal.SIGKILL)\n",
        "\n",
        "!wget -q https://mlg.eng.cam.ac.uk/teaching/4f13/2526/cw/cw1a.mat > /dev/null\n",
        "!wget -q https://mlg.eng.cam.ac.uk/teaching/4f13/2526/cw/cw1e.mat > /dev/null\n",
        "!wget -q https://raw.githubusercontent.com/cambridge-mlg/4f13-courseworks/refs/heads/main/2526/cw1/gp.py > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "AirLkLO6YzYe",
      "metadata": {
        "id": "AirLkLO6YzYe",
        "outputId": "d563d6e2-eca6-451f-ab5a-fe21d8dcb20a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'GPy'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1066246660.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mGPy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgp\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcw1utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'GPy'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import GPy\n",
        "import numpy as np\n",
        "import gp as cw1utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a339526a",
      "metadata": {
        "id": "a339526a"
      },
      "outputs": [],
      "source": [
        "### Arbitrary data, X and y\n",
        "X = np.random.rand(20, 1) * 10  # 20 data points in 1D\n",
        "y = np.sin(X) + (np.cos(X))**2 + np.random.randn(20, 1) * 0.5  # noisy opipbservations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3aced2d",
      "metadata": {
        "id": "c3aced2d"
      },
      "source": [
        "## Loading Matlab arrays in Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42d5f97c",
      "metadata": {
        "id": "42d5f97c"
      },
      "outputs": [],
      "source": [
        "import scipy.io as sio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "032ea1f3",
      "metadata": {
        "id": "032ea1f3"
      },
      "outputs": [],
      "source": [
        "\n",
        "data = sio.loadmat(\"cw1a.mat\")  # Note this will throw an error because the file does not exist!\n",
        "\n",
        "X = np.asarray(data[\"x\"], dtype=float)\n",
        "y = np.asarray(data[\"y\"], dtype=float)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e659d75",
      "metadata": {
        "id": "4e659d75"
      },
      "source": [
        "## GP Model Definition with Various Kernels (using GPy):"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd13e30a",
      "metadata": {
        "id": "cd13e30a"
      },
      "source": [
        "### Single kernels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77f1645d",
      "metadata": {
        "id": "77f1645d"
      },
      "source": [
        "Squared exponential (SE) isotropic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c9fd213",
      "metadata": {
        "id": "8c9fd213"
      },
      "outputs": [],
      "source": [
        "k = GPy.kern.RBF(input_dim=X.shape[1], lengthscale=1.0, variance=1.0)\n",
        "m = GPy.models.GPRegression(X, y, k)\n",
        "m.likelihood.variance = 1.0  # Note that this is the default value, so we don't strictly need to set it here unless we want a different initial value."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f182f6c",
      "metadata": {
        "id": "0f182f6c"
      },
      "source": [
        "Periodic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "821b9faf",
      "metadata": {
        "id": "821b9faf"
      },
      "outputs": [],
      "source": [
        "k = GPy.kern.StdPeriodic(input_dim=X.shape[1], lengthscale=1.0, period=1.0, variance=1.0)\n",
        "m = GPy.models.GPRegression(X, y, k)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05ef8f29",
      "metadata": {
        "id": "05ef8f29"
      },
      "source": [
        "SE Automatic Relevance Determination (ARD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bed6105e",
      "metadata": {
        "id": "bed6105e"
      },
      "outputs": [],
      "source": [
        "k = GPy.kern.RBF(input_dim=X.shape[1], ARD=True, lengthscale=1.0, variance=1.0)\n",
        "m = GPy.models.GPRegression(X, y, k)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a130e65",
      "metadata": {
        "id": "4a130e65"
      },
      "source": [
        "### Combined kernels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a19b16fb",
      "metadata": {
        "id": "a19b16fb"
      },
      "source": [
        "Product of Periodic and SE Isotropic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d197b123",
      "metadata": {
        "id": "d197b123"
      },
      "outputs": [],
      "source": [
        "k_per = GPy.kern.StdPeriodic(input_dim=X.shape[1], lengthscale=1.0, period=1.0, variance=1.0)\n",
        "k_se  = GPy.kern.RBF(input_dim=X.shape[1], lengthscale=1.0, variance=1.0)\n",
        "kernel = k_per * k_se\n",
        "m = GPy.models.GPRegression(X, y, kernel)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b998962",
      "metadata": {
        "id": "1b998962"
      },
      "source": [
        "Sum of two SE ARD Kernels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b4a20c4",
      "metadata": {
        "id": "4b4a20c4"
      },
      "outputs": [],
      "source": [
        "z = np.random.randn(4)\n",
        "ell1 = np.exp(z[0])\n",
        "sf1  = np.exp(z[1])\n",
        "var1 = sf1**2\n",
        "\n",
        "ell2 = np.exp(z[2])\n",
        "sf2  = np.exp(z[3])\n",
        "var2 = sf2**2\n",
        "\n",
        "k1 = GPy.kern.RBF(input_dim=X.shape[1], ARD=True, lengthscale=ell1, variance=var1)\n",
        "k2 = GPy.kern.RBF(input_dim=X.shape[1], ARD=True, lengthscale=ell2, variance=var2)\n",
        "k  = k1 + k2\n",
        "\n",
        "m = GPy.models.GPRegression(X, y, kernel=k)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ce816b7",
      "metadata": {
        "id": "4ce816b7"
      },
      "source": [
        "## GP Model Hyperparameter Optimisation using Marginal Likelihood"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3da5d30",
      "metadata": {
        "id": "b3da5d30"
      },
      "source": [
        "Once a GP regression model is specified, we can optimise its hyperparameters by minimising the negative log marginal likelihood (which is equivalent to maximising the likelihood of the data under the model.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f59ef468",
      "metadata": {
        "id": "f59ef468"
      },
      "outputs": [],
      "source": [
        "m.optimize(optimizer='lbfgsb', max_iters=2000, messages=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ac8f972",
      "metadata": {
        "id": "9ac8f972"
      },
      "source": [
        "## Utilities functions useful for producing figures to be included in report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4332448",
      "metadata": {
        "id": "d4332448"
      },
      "outputs": [],
      "source": [
        "cw1utils.print_model_summary(m)\n",
        "cw1utils.print_model_marginal_likelihood(m)\n",
        "cw1utils.plot_predictive_error_bars(m, X, y, \"\\n command\\n another command\")\n",
        "\n",
        "# cw1utils.plot_2d_predictive(mA, mB, codeA=None, codeB=None, save_path=None)\n",
        "# cw1utils.plot_sampled_functions(X, F, code_snippet, n_draws=3, save_path=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now to the questions"
      ],
      "metadata": {
        "id": "MNZ0hrmZpkw8"
      },
      "id": "MNZ0hrmZpkw8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question A"
      ],
      "metadata": {
        "id": "PDWs-szwpnHd"
      },
      "id": "PDWs-szwpnHd"
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the data\n",
        "\n",
        "data = sio.loadmat(\"cw1a.mat\")\n",
        "\n",
        "X = np.asarray(data[\"x\"], dtype=float)\n",
        "y = np.asarray(data[\"y\"], dtype=float)\n",
        "\n",
        "#  Train a GP with a squared exponential covariance function, GPy.kern.RBF() and intialise the hyper parameters as described.\n",
        "\n",
        "k = GPy.kern.RBF(input_dim=X.shape[1], lengthscale=np.exp(-1.0), variance=1.0)\n",
        "m = GPy.models.GPRegression(X, y, k)\n",
        "\n",
        "m.Gaussian_noise.variance = 1.0"
      ],
      "metadata": {
        "id": "xVMGjn3Qpr9p"
      },
      "id": "xVMGjn3Qpr9p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m.optimize()"
      ],
      "metadata": {
        "id": "ICV1jdu2psz7"
      },
      "id": "ICV1jdu2psz7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(m)"
      ],
      "metadata": {
        "id": "nScAdH5Sp-MM"
      },
      "id": "nScAdH5Sp-MM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cw1utils.plot_predictive_error_bars(m, X, y, \"\\nk = GPy.kern.RBF(input_dim=X.shape[1], lengthscale=np.exp(-1.0), variance=1.0) \\nm = GPy.models.GPRegression(X, y, k) \\nm.Gaussian_noise.variance = 1.0 \\nm.optimize() \")"
      ],
      "metadata": {
        "id": "qLJt7kVFqEK6"
      },
      "id": "qLJt7kVFqEK6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some preliminary observations for this part a"
      ],
      "metadata": {
        "id": "3IKei-mJF9Lk"
      },
      "id": "3IKei-mJF9Lk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysing the optimised params:\n",
        "---\n",
        "\n",
        "- lengthscale: This is a very small value relative to the input range.\n",
        "- The model now assumes points only a small distance apart are highly correlated. This allows the model to produce a highly oscillating or \"wiggly\" function that accurately tracks the rapid variations in the data.\n",
        "- Low noise estimate: The model believes the data is relatively clean and the wiggles are true signal, not just noise. It therefore attempts to pass very closely to the majority of the data points.\n",
        "- This value (variance $\\sigma_f^2 \\approx 0.805$) is close to the vertical range of the wiggles.Consequence: It confirms the magnitude of the smooth variations that the function makes.\n",
        "\n",
        "- Narrow Bands Near Data (Interpolation): The shaded area is tightly constrained around the predictive mean (blue line) where the training data (red crosses) exists. This narrowness is due to the low noise estimate ($\\sigma_n \\approx 0.118$), showing high confidence in the function's value near observed points.\n",
        "- The small optimized lengthscale ($\\ell=0.128$) implies that the function must be highly wiggly. This is clearly demonstrated by the predictive error bars, which are extremely narrow directly around the data points (due to the low noise $\\sigma_n$), but rapidly widen in any region devoid of data (e.g., $x \\approx -2.5$ and $x>2.0$), because the model instantly loses correlation just a short distance away from the observations.\n",
        "- Uncertainty Spikes in Extrapolation: In the regions far outside the data (e.g., $x>2.5$ or $x<-3.0$), the uncertainty quickly reaches the maximum level defined by the Signal Standard Deviation, confirming the model has no idea what happens far outside the training range.\n"
      ],
      "metadata": {
        "id": "Uwzx0WVKGBU5"
      },
      "id": "Uwzx0WVKGBU5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mental Cheatsheet\n",
        "- **Lengthscale (l)**: small value - wiggly fxn, quickly changing. High value - Smooth function, slow to change. low l is typically less than input range. high l is higher than input range.\n",
        "\n",
        "- **Noise Variance ($\\sigma_n^2$) (The Cleanliness Measure):** The Noise Variance is compared to the Signal Variance ($\\sigma_f^2$) itself, as it defines the signal-to-noise ratio. Low $\\sigma_n^2$ that is $\\sigma_n^2 \\ll \\sigma_f^2$ means *the data is believed to be clean*. The model trusts the data points and tries to fit them tightly, resulting in narrow error bars at the data points. For $\\sigma_n^2 \\approx \\sigma_f^2$ or $\\sigma_n^2 > \\sigma_f^2$, The data is believed to be noisy. The model is forced to smooth over the points, and the uncertainty remains high everywhere. *In summary, if low, data is clean; model fits points tightly. If high, data is noisy; model smooths over the points.*\n",
        "\n",
        "- **Signal variance**: If it is low, function is flat. If it is high, function has large vertical swings."
      ],
      "metadata": {
        "id": "9DYMDVbIJJYb"
      },
      "id": "9DYMDVbIJJYb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question B\n"
      ],
      "metadata": {
        "id": "lQHiRvsQI2rA"
      },
      "id": "lQHiRvsQI2rA"
    },
    {
      "cell_type": "code",
      "source": [
        "print(m)"
      ],
      "metadata": {
        "id": "B8YK22qhqnQE"
      },
      "id": "B8YK22qhqnQE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m.optimize_restarts(num_restarts=20, verbose=True)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "coJv1q3zJ1bN"
      },
      "id": "coJv1q3zJ1bN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(m)"
      ],
      "metadata": {
        "id": "OiXBaCTTLBJe"
      },
      "id": "OiXBaCTTLBJe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = sio.loadmat(\"cw1a.mat\")\n",
        "\n",
        "X = np.asarray(data[\"x\"], dtype=float)\n",
        "y = np.asarray(data[\"y\"], dtype=float)\n",
        "\n",
        "#  Train a GP with a squared exponential covariance function, GPy.kern.RBF() and intialise the hyper parameters as described.\n",
        "\n",
        "k = GPy.kern.RBF(input_dim=X.shape[1], lengthscale=np.exp(-1.0), variance=1.0)\n",
        "m = GPy.models.GPRegression(X, y, k)\n",
        "\n",
        "m.Gaussian_noise.variance = 1.0\n",
        "m.optimize()\n",
        "cw1utils.plot_predictive_error_bars(m, X, y, \"\\nk= GPy.kern.RBF(input_dim=X.shape[1], lengthscale=np.exp(-1.0), variance=1.0)\\nm = GPy.models.GPRegression(X, y, k)\\nm.Gaussian_noise.variance = 1.0\\nm.optimize()\")"
      ],
      "metadata": {
        "id": "Cx3zXJ6GDkTU"
      },
      "id": "Cx3zXJ6GDkTU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model B is the Local Optimum (NLML ≈ 78.220)\n",
        "\n",
        "# 1. Define a NEW model (m_local) and initialize to the \"smooth\" basin\n",
        "k_local = GPy.kern.RBF(input_dim=X.shape[1], variance=1.0, lengthscale=10.0)\n",
        "m_local = GPy.models.GPRegression(X, y, k_local)\n",
        "m_local.Gaussian_noise.variance = 10.0\n",
        "\n",
        "m_local.optimize()\n",
        "\n",
        "# 3. Print the final hyperparameters and NLML (Should show NLML ≈ 78.220)\n",
        "print(\"\\n--- Model B: Local Optimum ---\")\n",
        "print(m_local)\n",
        "\n",
        "# 4. Plot the poor fit (Smooth/Noisy interpretation)\n",
        "# This plot shows the flat mean function and wide, constant error bars.\n",
        "cw1utils.plot_predictive_error_bars(m_local, X, y, \"\\nk_local = GPy.kern.RBF(input_dim=X.shape[1], variance=1.0, lengthscale=10.0)\\nm_local = GPy.models.GPRegression(X, y, k_local)\\nm_local.Gaussian_noise.variance = 10.0\\nm_local.optimize()\")"
      ],
      "metadata": {
        "id": "wSXHGpg7I5Wp"
      },
      "id": "wSXHGpg7I5Wp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Potential answers\n",
        "---\n",
        "\n",
        "Multiple optimization restarts confirmed the existence of two distinct local optima. Specifically, when initializing the hyperparameters to find the Smooth/Noisy Optimum (Optimum B), we observed that the Negative Log Marginal Likelihood (NLML) consistently converged to $\\mathbf{78.220}$, even with varied initial values for the kernel variance ($\\sigma_f^2$) and noise variance ($\\sigma_n^2$).\n",
        "\n",
        "For 78.22: \"This stability is due to the very large initial lengthscale ($\\ell=10.0$) dominating the model's interpretation. A large lengthscale locks the model into the assumption that the underlying function is extremely smooth (i.e., almost flat). Once committed to this smooth structure, the optimization problem simplifies. The optimizer merely finds the optimal ratio between $\\sigma_f^2$ and $\\sigma_n^2$ necessary to account for the data scatter under that fixed, smooth assumption. This local minimum is a very wide and shallow valley in the NLML landscape, making it easy to find and highly stable, regardless of the initial variance settings.\"\n",
        "\n",
        "For 11.899: In contrast, the Global Optimum (A), with $\\text{NLML} \\approx 11.899$, requires the lengthscale to be optimized to a very small value ($\\ell \\approx 0.128$). This optimal NLML region is much sharper and harder to find, which is why the initial configuration of $\\ell=e^{-1}$ only sometimes lands there, leading to a much greater $\\Delta \\text{NLML}$ and high confidence in its superiority."
      ],
      "metadata": {
        "id": "fSF1MeXa70v4"
      },
      "id": "fSF1MeXa70v4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question C"
      ],
      "metadata": {
        "id": "NB0VgPBAtFoy"
      },
      "id": "NB0VgPBAtFoy"
    },
    {
      "cell_type": "code",
      "source": [
        "k = GPy.kern.StdPeriodic(input_dim=X.shape[1], lengthscale=1, period=1.0, variance=1.0)\n",
        "m = GPy.models.GPRegression(X, y, k)"
      ],
      "metadata": {
        "id": "SkNYRg6FtFPj"
      },
      "id": "SkNYRg6FtFPj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m.optimize()"
      ],
      "metadata": {
        "id": "z5GrSO1ft0dq"
      },
      "id": "z5GrSO1ft0dq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cw1utils.plot_predictive_error_bars(m, X, y, \"\\nk = GPy.kern.StdPeriodic(input_dim=X.shape[1], lengthscale=1, period=1.0, variance=1.0) \\nm = GPy.models.GPRegression(X, y, k)\\nm.optimize()\")"
      ],
      "metadata": {
        "id": "7hHC1x4Xt1B6"
      },
      "id": "7hHC1x4Xt1B6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Potential answer\n",
        "---\n",
        "\n",
        "The evidence overwhelmingly supports the conclusion that the underlying data-generating function is strictly periodic in nature.\n",
        "\n",
        "1. Quantitative Evidence (Model Selection): The most compelling evidence comes from the Negative Log Marginal Likelihood (NLML), which is the gold standard for model comparison:RBF (Smooth/Wiggly) NLML: $11.899$ Periodic NLML: $\\mathbf{-35.267}$ The difference, $\\Delta \\text{NLML} \\approx 47.166$, is a massive reduction. This tells us the periodic assumption provides a vastly superior and simpler explanation of the data's structure. If the data were only \"quasi-periodic\" (just generally wiggling), the flexible RBF kernel would likely have achieved a much closer NLML score. The fact that the strictly defined Periodic kernel wins so decisively proves the repeating pattern is highly regular.\n",
        "\n",
        "2. Hyperparameter Evidence: The optimized hyperparameters confirm the strictness of the pattern:Clean Period ($p$): The model optimized the period to $\\mathbf{0.999}$. This is almost mathematically equal to $1.0$, indicating the spacing of the wiggles is extremely regular and consistent across the entire input domain.Low Noise ($\\sigma_n$): The final noise standard deviation of $\\mathbf{0.110}$ means the model assumes very little error is left over after fitting the periodic function. The remaining error is attributed only to minor observation noise, not to failures in the periodic structure itself.\n",
        "\n",
        "3. Qualitative Evidence (Error Bars): The behavior of the predictive error bars qualitatively validates the periodic assumption:The uncertainty band is narrow everywhere it is constrained by the fitted function.Crucially, in the extrapolation regions (e.g., $x>2.5$), the predictive mean continues the oscillation indefinitely, and the uncertainty remains low. This demonstrates the model's high confidence that the pattern repeats exactly into unobserved territory, which is the very definition of a strictly periodic function.\n",
        "\n",
        "4. Conclusion: The combination of a highly regular optimized period ($p \\approx 1.0$) and the overwhelming statistical preference for the Periodic model ($\\text{NLML} = -35.267$) provides conclusive evidence that the data-generating mechanism, apart from noise, is strictly periodic."
      ],
      "metadata": {
        "id": "Xr6F6B3z05UY"
      },
      "id": "Xr6F6B3z05UY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question D"
      ],
      "metadata": {
        "id": "8NmgCKkG2kK_"
      },
      "id": "8NmgCKkG2kK_"
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = np.linspace(-5.0, 5.0, 200).reshape(-1, 1)\n",
        "k_se = GPy.kern.RBF(input_dim=1, variance=1.0, lengthscale=np.exp(2.0))\n",
        "k_per = GPy.kern.StdPeriodic(input_dim=1, variance=1.0, lengthscale=np.exp(-0.5), period=1.0)\n",
        "kernel = k_per * k_se\n",
        "# m = GPy.models.GPRegression(X, y, kernel)\n",
        "\n",
        "m_sample = GPy.models.GPRegression(X_test, np.zeros_like(X_test), kernel)\n",
        "m_sample.Gaussian_noise.variance.fix(1e-6)"
      ],
      "metadata": {
        "id": "4eli4kVw1aj8"
      },
      "id": "4eli4kVw1aj8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m_sample.optimize()"
      ],
      "metadata": {
        "id": "KA0heRor3vPK"
      },
      "id": "KA0heRor3vPK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cw1utils.plot_predictive_error_bars(m_sample, X_test, np.zeros_like(X_test), \"\\n command\\n another command\")"
      ],
      "metadata": {
        "id": "enR_1PBP41GP"
      },
      "id": "enR_1PBP41GP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define X\n",
        "X = np.linspace(-5.0, 5.0, 200, dtype=np.float64).reshape(-1, 1)\n",
        "\n",
        "# Create product kernel: RBF * Periodic\n",
        "kern_rbf = GPy.kern.RBF(input_dim=1, lengthscale=np.exp(2), variance=1.0)\n",
        "kern_periodic = GPy.kern.StdPeriodic(input_dim=1, lengthscale=np.exp(-0.5),\n",
        "                                      period=1.0, variance=1.0)\n",
        "kern_product = kern_rbf * kern_periodic\n",
        "\n",
        "# Compute covariance matrix\n",
        "K = kern_product.K(X,X)\n",
        "\n",
        "# Add small jitter for numerical stability\n",
        "jitter = 1e-6 * np.eye(200)\n",
        "K_stable = K + jitter\n",
        "\n",
        "# Cholesky decomposition\n",
        "L = np.linalg.cholesky(K_stable)\n",
        "\n",
        "# Generate random samples\n",
        "n_samples = 4\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i in range(n_samples):\n",
        "    u = np.random.randn(200, 1)\n",
        "    f_sample = L @ u\n",
        "    plt.plot(X, f_sample, alpha=0.7, label=f'Sample {i+1}')\n",
        "\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('f(X)')\n",
        "plt.title('Random Functions from RBF x Periodic GP')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LaLd6-Ae5VRG"
      },
      "id": "LaLd6-Ae5VRG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import GPy\n",
        "\n",
        "# Define X\n",
        "X = np.linspace(-5.0, 5.0, 200, dtype=np.float64).reshape(-1, 1)\n",
        "\n",
        "# Create product kernel: RBF * Periodic\n",
        "kern_rbf = GPy.kern.RBF(input_dim=1, lengthscale=np.exp(2), variance=1.0)\n",
        "kern_periodic = GPy.kern.StdPeriodic(input_dim=1, lengthscale=np.exp(-0.5),\n",
        "                                      period=1.0, variance=1.0)\n",
        "kern_product = kern_rbf * kern_periodic\n",
        "\n",
        "# Compute covariance matrix\n",
        "K = kern_product.K(X, X)\n",
        "\n",
        "# Add small jitter for numerical stability\n",
        "jitter = 1e-6 * np.eye(200)\n",
        "K_stable = K + jitter\n",
        "\n",
        "# Cholesky decomposition\n",
        "L = np.linalg.cholesky(K_stable)\n",
        "\n",
        "# Generate random samples (matrix with columns as samples)\n",
        "n_samples = 5\n",
        "F = L @ np.random.randn(200, n_samples)  # Shape: (200, n_samples)\n",
        "\n",
        "# Code snippet for display\n",
        "code_snippet = \"\"\"\n",
        "kern = GPy.kern.RBF(1, lengthscale=np.exp(2), variance=1.0) * \\\\\n",
        "       GPy.kern.StdPeriodic(1, lengthscale=np.exp(-0.5), period=1.0, variance=1.0)\n",
        "K = kern.K(X) + 1e-6 * np.eye(200)\n",
        "L = np.linalg.cholesky(K)\n",
        "F = L @ np.random.randn(200, n_samples)\n",
        "\"\"\"\n",
        "\n",
        "# Use professor's plotting utility\n",
        "cw1utils.plot_sampled_functions(X, F, code_snippet, n_draws=5)"
      ],
      "metadata": {
        "id": "lJn59BsytKZa"
      },
      "id": "lJn59BsytKZa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question E"
      ],
      "metadata": {
        "id": "CGqvh5ra6kgs"
      },
      "id": "CGqvh5ra6kgs"
    },
    {
      "cell_type": "code",
      "source": [
        "data = sio.loadmat(\"cw1e.mat\")\n",
        "# Note: Check the actual keys in the .mat file. Assuming 'x' and 'y' based on common practice.\n",
        "X = np.asarray(data[\"x\"], dtype=float)\n",
        "Y = np.asarray(data[\"y\"], dtype=float)\n",
        "\n",
        "# Reshape 1D arrays into 11x11 grids for the plot_surface function\n",
        "X1_grid = X[:,0].reshape(11, 11)\n",
        "X2_grid = X[:,1].reshape(11, 11)\n",
        "Y_surface = Y.reshape(11, 11)\n",
        "\n",
        "# --- Create 3D Plots Side-by-Side ---\n",
        "fig = plt.figure(figsize=(15, 7)) # Increase size to fit two plots horizontally\n",
        "\n",
        "# Plot 1: Standard View (Position 1 of 2)\n",
        "ax1 = fig.add_subplot(1, 2, 1, projection='3d') # Corrected: 1 row, 2 columns, plot 1\n",
        "ax1.plot_surface(X1_grid, X2_grid, Y_surface, cmap='viridis')\n",
        "ax1.set_xlabel('X[:,0] Input')\n",
        "ax1.set_ylabel('X[:,1] Input')\n",
        "ax1.set_zlabel('Output Y')\n",
        "ax1.set_title('Standard View')\n",
        "\n",
        "# Plot 2: Rotated View (Position 2 of 2)\n",
        "ax2 = fig.add_subplot(1, 2, 2, projection='3d') # Corrected: 1 row, 2 columns, plot 2\n",
        "ax2.plot_surface(X1_grid, X2_grid, Y_surface, cmap='viridis')\n",
        "\n",
        "# Use ax.view_init() to rotate the view (e.g., elevation 30 degrees, azimuth 60 degrees)\n",
        "# This fulfills the requirement to \"Rotate the data, to get a feel for it.\"\n",
        "ax2.view_init(elev=30, azim=60)\n",
        "\n",
        "ax2.set_xlabel('X[:,0] Input')\n",
        "ax2.set_ylabel('X[:,1] Input')\n",
        "ax2.set_zlabel('Output Y (Rotated)')\n",
        "ax2.set_title('Rotated View')\n",
        "\n",
        "plt.tight_layout() # Adjusts spacing between subplots\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IkquKzBU58Nl"
      },
      "id": "IkquKzBU58Nl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model A: Single ARD RBF\n",
        "k_A = GPy.kern.RBF(input_dim=2, ARD=True) # ARD=True gives two lengthscales\n",
        "m_A = GPy.models.GPRegression(X, Y, k_A)\n",
        "m_A.optimize_restarts(num_restarts=10)\n",
        "NLML_A = m_A.log_likelihood() * -1\n",
        "\n",
        "# Model B: Sum of two ARD RBF Kernels\n",
        "# Break symmetry with random initial values (as suggested)\n",
        "rands = 0.1 * np.random.randn(6) # Random initial values for 2 variances and 4 lengthscales\n",
        "\n",
        "k1 = GPy.kern.RBF(input_dim=2, ARD=True, variance=rands[0]**2, lengthscale=np.exp(rands[1:3]))\n",
        "k2 = GPy.kern.RBF(input_dim=2, ARD=True, variance=rands[3]**2, lengthscale=np.exp(rands[4:6]))\n",
        "k_B = k1 + k2\n",
        "\n",
        "m_B = GPy.models.GPRegression(X, Y, k_B)\n",
        "m_B.optimize_restarts(num_restarts=10)\n",
        "NLML_B = m_B.log_likelihood() * -1"
      ],
      "metadata": {
        "id": "Z-Qpmb7Y8A-5"
      },
      "id": "Z-Qpmb7Y8A-5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bnbXADX_nw9C"
      },
      "id": "bnbXADX_nw9C",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}